{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 165,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import spacy"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-12T23:26:27.283871235Z",
     "start_time": "2023-07-12T23:26:27.280453749Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 19 entries, 0 to 18\n",
      "Columns: 118 entries, parent to category\n",
      "dtypes: float64(72), object(46)\n",
      "memory usage: 17.6+ KB\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_excel(r'./DatabaseTableUpload/Appendix_A_Capstone_DataSharingProposal.xlsx', sheet_name='A.25_ServiceNow_Incidents') # Provide path for a single file.\n",
    "df.info()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-12T23:26:27.532505035Z",
     "start_time": "2023-07-12T23:26:27.437795098Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "### Named Entity Recognition (NER)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Sizes: ['sm', 'md', 'lg']<br>\n",
    "Your pipeline must be compatable with your current version of SpaCy.\n",
    "\n",
    "Can download the following on Conda (base) environment:\n",
    "`pip install https://github.com/explosion/spacy-models/releases/download/en_core_web_[SIZE]-[VERSION]/en_core_web_md-[VERSION].tar.gz`\n",
    "\n",
    "Alternatively, in Python:\n",
    "`python -m spacy download en_core_web_sm`"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "outputs": [],
   "source": [
    "from spacy.lang.en import English\n",
    "nlp = English()\n",
    "\n",
    "# Instantiate a Tokenizer with the default settings for English, including punctuation rules and exceptions.\n",
    "tokenizer = nlp.tokenizer"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-12T23:26:27.972321979Z",
     "start_time": "2023-07-12T23:26:27.884832686Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['transformer', 'tagger', 'parser', 'attribute_ruler', 'lemmatizer', 'ner']\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.load('en_core_web_trf') # Load transfomer model.\n",
    "print(nlp.pipe_names)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-12T23:26:30.933964135Z",
     "start_time": "2023-07-12T23:26:28.047098203Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "#### Enhance the transformer NER model with added examples.\n",
    "#### https://www.machinelearningplus.com/nlp/training-custom-ner-model-in-spacy/\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17\n"
     ]
    }
   ],
   "source": [
    "text_test = \"Microsoft Excel throws error 0xC0000142\"\n",
    "phrase_len = len(\"Microsoft Outlook\")\n",
    "text_test[0:15]\n",
    "print(phrase_len)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-12T23:26:30.934599732Z",
     "start_time": "2023-07-12T23:26:30.915982172Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "outputs": [],
   "source": [
    "# Training examples - do this only for *individual* NERs, e.g. single-word NERs.\n",
    "train_data = [\n",
    "    (\"Windows Defender blocked my internet access this morning, can you take a look?\", {\"entities\": [(0, 16, \"SOFTWARE\")]}),\n",
    "    (\"BitLocker malfunctioned upon scanning this morning\", {\"entities\": [(0,9, \"SOFTWARE\")]}),\n",
    "    (\"Please check to see if Windows Firewall is working\", {\"entities\": [(23,39, \"SOFTWARE\")]}),\n",
    "    (\"Baird TrustDesk Migration to OneDrive - Error migrating\", {\"entities\": [(0, 15, \"SECURITY\"), (29, 37, \"SOFTWARE\")]}),\n",
    "    (\"Hello. My name is spelled incorrectly on the DocuSign application. It is Veronica Fitzpatrick, some letters are all flipped around  backwards and I was hoping to have that fixed!\", {\"entities\": [(45, 53, \"SOFTWARE\"), (73, 93, \"PERSON\")]}),\n",
    "    (\"I use Microsoft Office for my daily work\", {\"entities\": [(6, 22, \"SOFTWARE\")]}),\n",
    "    (\"Microsoft Teams is crashing\", {\"entities\": [(0, 15, \"SOFTWARE\")]}),\n",
    "    (\"Microsoft Outlook is crashing\", {\"entities\": [(0, 17, \"SOFTWARE\")]}),\n",
    "    (\"Microsoft Excel throws error 0xC0000142\", {\"entities\": [(0, 15, \"SOFTWARE\")]})\n",
    "]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-12T23:26:30.934706564Z",
     "start_time": "2023-07-12T23:26:30.916255113Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "outputs": [],
   "source": [
    "# Adding labels to the `ner`\n",
    "# for _, annotations in train_data:\n",
    "#     for ent in annotations.get(\"entities\"):\n",
    "#         ner.add_label(ent[2])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-12T23:26:30.934775173Z",
     "start_time": "2023-07-12T23:26:30.916412947Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Input \u001B[0;32mIn [172]\u001B[0m, in \u001B[0;36m<cell line: 56>\u001B[0;34m()\u001B[0m\n\u001B[1;32m     65\u001B[0m                 example \u001B[38;5;241m=\u001B[39m Example\u001B[38;5;241m.\u001B[39mfrom_dict(doc, annotations)\n\u001B[1;32m     66\u001B[0m                 examples\u001B[38;5;241m.\u001B[39mappend(example)\n\u001B[0;32m---> 67\u001B[0m             \u001B[43mnlp\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mupdate\u001B[49m\u001B[43m(\u001B[49m\u001B[43mexamples\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mexamples\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlosses\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mlosses\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdrop\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m0.4\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m     68\u001B[0m         \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mIteration:\u001B[39m\u001B[38;5;124m\"\u001B[39m, itn \u001B[38;5;241m+\u001B[39m \u001B[38;5;241m1\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mLoss:\u001B[39m\u001B[38;5;124m\"\u001B[39m, losses)\n\u001B[1;32m     70\u001B[0m nlp\u001B[38;5;241m.\u001B[39mto_disk(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m./en_web_core_trf_updated\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "File \u001B[0;32m~/anaconda3/lib/python3.9/site-packages/spacy/language.py:1156\u001B[0m, in \u001B[0;36mLanguage.update\u001B[0;34m(self, examples, _, drop, sgd, losses, component_cfg, exclude, annotates)\u001B[0m\n\u001B[1;32m   1153\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m name, proc \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpipeline:\n\u001B[1;32m   1154\u001B[0m     \u001B[38;5;66;03m# ignore statements are used here because mypy ignores hasattr\u001B[39;00m\n\u001B[1;32m   1155\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m name \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m exclude \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(proc, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mupdate\u001B[39m\u001B[38;5;124m\"\u001B[39m):\n\u001B[0;32m-> 1156\u001B[0m         \u001B[43mproc\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mupdate\u001B[49m\u001B[43m(\u001B[49m\u001B[43mexamples\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msgd\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlosses\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mlosses\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mcomponent_cfg\u001B[49m\u001B[43m[\u001B[49m\u001B[43mname\u001B[49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m  \u001B[38;5;66;03m# type: ignore\u001B[39;00m\n\u001B[1;32m   1157\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m sgd \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m (\u001B[38;5;28;01mNone\u001B[39;00m, \u001B[38;5;28;01mFalse\u001B[39;00m):\n\u001B[1;32m   1158\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m (\n\u001B[1;32m   1159\u001B[0m             name \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m exclude\n\u001B[1;32m   1160\u001B[0m             \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(proc, ty\u001B[38;5;241m.\u001B[39mTrainableComponent)\n\u001B[1;32m   1161\u001B[0m             \u001B[38;5;129;01mand\u001B[39;00m proc\u001B[38;5;241m.\u001B[39mis_trainable\n\u001B[1;32m   1162\u001B[0m             \u001B[38;5;129;01mand\u001B[39;00m proc\u001B[38;5;241m.\u001B[39mmodel \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m (\u001B[38;5;28;01mTrue\u001B[39;00m, \u001B[38;5;28;01mFalse\u001B[39;00m, \u001B[38;5;28;01mNone\u001B[39;00m)\n\u001B[1;32m   1163\u001B[0m         ):\n",
      "File \u001B[0;32m~/anaconda3/lib/python3.9/site-packages/spacy/pipeline/transition_parser.pyx:419\u001B[0m, in \u001B[0;36mspacy.pipeline.transition_parser.Parser.update\u001B[0;34m()\u001B[0m\n",
      "File \u001B[0;32m~/anaconda3/lib/python3.9/site-packages/spacy/ml/parser_model.pyx:300\u001B[0m, in \u001B[0;36mspacy.ml.parser_model.ParserStepModel.finish_steps\u001B[0;34m()\u001B[0m\n",
      "File \u001B[0;32m~/anaconda3/lib/python3.9/site-packages/thinc/layers/chain.py:60\u001B[0m, in \u001B[0;36mforward.<locals>.backprop\u001B[0;34m(dY)\u001B[0m\n\u001B[1;32m     58\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mbackprop\u001B[39m(dY: OutT) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m InT:\n\u001B[1;32m     59\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m callback \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mreversed\u001B[39m(callbacks):\n\u001B[0;32m---> 60\u001B[0m         dX \u001B[38;5;241m=\u001B[39m \u001B[43mcallback\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdY\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     61\u001B[0m         dY \u001B[38;5;241m=\u001B[39m dX\n\u001B[1;32m     62\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m dX\n",
      "File \u001B[0;32m~/anaconda3/lib/python3.9/site-packages/thinc/layers/chain.py:60\u001B[0m, in \u001B[0;36mforward.<locals>.backprop\u001B[0;34m(dY)\u001B[0m\n\u001B[1;32m     58\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mbackprop\u001B[39m(dY: OutT) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m InT:\n\u001B[1;32m     59\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m callback \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mreversed\u001B[39m(callbacks):\n\u001B[0;32m---> 60\u001B[0m         dX \u001B[38;5;241m=\u001B[39m \u001B[43mcallback\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdY\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     61\u001B[0m         dY \u001B[38;5;241m=\u001B[39m dX\n\u001B[1;32m     62\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m dX\n",
      "File \u001B[0;32m~/anaconda3/lib/python3.9/site-packages/spacy_transformers/layers/listener.py:40\u001B[0m, in \u001B[0;36mTransformerListener.backprop_and_clear\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m     35\u001B[0m \u001B[38;5;124;03m\"\"\"Call the stored _backprop callback, and then\u001B[39;00m\n\u001B[1;32m     36\u001B[0m \u001B[38;5;124;03mclears it. This saves memory, as otherwise we hold onto that callback\u001B[39;00m\n\u001B[1;32m     37\u001B[0m \u001B[38;5;124;03muntil the next batch.\u001B[39;00m\n\u001B[1;32m     38\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m     39\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backprop \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m---> 40\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_backprop\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     41\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m     42\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[0;32m~/anaconda3/lib/python3.9/site-packages/spacy_transformers/pipeline_component.py:320\u001B[0m, in \u001B[0;36mTransformer.update.<locals>.backprop\u001B[0;34m(d_trf_datas)\u001B[0m\n\u001B[1;32m    318\u001B[0m accumulate_gradient(d_trf_datas)\n\u001B[1;32m    319\u001B[0m d_trf_full \u001B[38;5;241m=\u001B[39m trf_full\u001B[38;5;241m.\u001B[39munsplit_by_doc(d_tensors)\n\u001B[0;32m--> 320\u001B[0m d_docs \u001B[38;5;241m=\u001B[39m \u001B[43mbp_trf_full\u001B[49m\u001B[43m(\u001B[49m\u001B[43md_trf_full\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    321\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m sgd \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    322\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmodel\u001B[38;5;241m.\u001B[39mfinish_update(sgd)\n",
      "File \u001B[0;32m~/anaconda3/lib/python3.9/site-packages/spacy_transformers/layers/transformer_model.py:200\u001B[0m, in \u001B[0;36mforward.<locals>.backprop_transformer\u001B[0;34m(d_output)\u001B[0m\n\u001B[1;32m    198\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mlogger\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01min\u001B[39;00m model\u001B[38;5;241m.\u001B[39mattrs:\n\u001B[1;32m    199\u001B[0m     log_gpu_memory(model\u001B[38;5;241m.\u001B[39mattrs[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mlogger\u001B[39m\u001B[38;5;124m\"\u001B[39m], \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mBegin backprop\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m--> 200\u001B[0m _ \u001B[38;5;241m=\u001B[39m \u001B[43mbp_tensors\u001B[49m\u001B[43m(\u001B[49m\u001B[43md_output\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmodel_output\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    201\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mlogger\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01min\u001B[39;00m model\u001B[38;5;241m.\u001B[39mattrs:\n\u001B[1;32m    202\u001B[0m     log_gpu_memory(model\u001B[38;5;241m.\u001B[39mattrs[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mlogger\u001B[39m\u001B[38;5;124m\"\u001B[39m], \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mAfter backprop\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "File \u001B[0;32m~/anaconda3/lib/python3.9/site-packages/thinc/layers/pytorchwrapper.py:139\u001B[0m, in \u001B[0;36mforward.<locals>.backprop\u001B[0;34m(dY)\u001B[0m\n\u001B[1;32m    137\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mbackprop\u001B[39m(dY: Any) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Any:\n\u001B[1;32m    138\u001B[0m     dYtorch \u001B[38;5;241m=\u001B[39m get_dYtorch(dY)\n\u001B[0;32m--> 139\u001B[0m     dXtorch \u001B[38;5;241m=\u001B[39m \u001B[43mtorch_backprop\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdYtorch\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    140\u001B[0m     dX \u001B[38;5;241m=\u001B[39m get_dX(dXtorch)\n\u001B[1;32m    141\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m dX\n",
      "File \u001B[0;32m~/anaconda3/lib/python3.9/site-packages/thinc/shims/pytorch.py:109\u001B[0m, in \u001B[0;36mPyTorchShim.begin_update.<locals>.backprop\u001B[0;34m(grads)\u001B[0m\n\u001B[1;32m     98\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mbackprop\u001B[39m(grads):\n\u001B[1;32m     99\u001B[0m     \u001B[38;5;66;03m# Normally, gradient scaling is applied to the loss of a model. However,\u001B[39;00m\n\u001B[1;32m    100\u001B[0m     \u001B[38;5;66;03m# since regular thinc layers do not use mixed-precision, we perform scaling\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    103\u001B[0m     \u001B[38;5;66;03m# backprop'ed through the succeeding layer to get the same effect as loss\u001B[39;00m\n\u001B[1;32m    104\u001B[0m     \u001B[38;5;66;03m# scaling.\u001B[39;00m\n\u001B[1;32m    105\u001B[0m     grads\u001B[38;5;241m.\u001B[39mkwargs[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mgrad_tensors\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_grad_scaler\u001B[38;5;241m.\u001B[39mscale(\n\u001B[1;32m    106\u001B[0m         grads\u001B[38;5;241m.\u001B[39mkwargs[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mgrad_tensors\u001B[39m\u001B[38;5;124m\"\u001B[39m], inplace\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[1;32m    107\u001B[0m     )\n\u001B[0;32m--> 109\u001B[0m     \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mautograd\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mgrads\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mgrads\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    111\u001B[0m     \u001B[38;5;66;03m# Unscale weights and check for overflows during backprop.\u001B[39;00m\n\u001B[1;32m    112\u001B[0m     grad_tensors \u001B[38;5;241m=\u001B[39m []\n",
      "File \u001B[0;32m~/anaconda3/lib/python3.9/site-packages/torch/autograd/__init__.py:200\u001B[0m, in \u001B[0;36mbackward\u001B[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001B[0m\n\u001B[1;32m    195\u001B[0m     retain_graph \u001B[38;5;241m=\u001B[39m create_graph\n\u001B[1;32m    197\u001B[0m \u001B[38;5;66;03m# The reason we repeat same the comment below is that\u001B[39;00m\n\u001B[1;32m    198\u001B[0m \u001B[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001B[39;00m\n\u001B[1;32m    199\u001B[0m \u001B[38;5;66;03m# calls in the traceback and some print out the last line\u001B[39;00m\n\u001B[0;32m--> 200\u001B[0m \u001B[43mVariable\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_execution_engine\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrun_backward\u001B[49m\u001B[43m(\u001B[49m\u001B[43m  \u001B[49m\u001B[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001B[39;49;00m\n\u001B[1;32m    201\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtensors\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgrad_tensors_\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    202\u001B[0m \u001B[43m    \u001B[49m\u001B[43mallow_unreachable\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43maccumulate_grad\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "from spacy.training.example import Example\n",
    "import random\n",
    "\n",
    "ner = nlp.get_pipe(\"ner\")\n",
    "\n",
    "# TRAINING THE MODEL\n",
    "# Disable pipeline components you don't need to change.\n",
    "\n",
    "# METHOD 1.\n",
    "# other_pipes = [pipe for pipe in nlp.pipe_names if pipe != \"ner\"]\n",
    "# with nlp.select_pipes(disable=[\"tagger\", \"parser\", \"attribute_ruler\", \"lemmatizer\"]):\n",
    "#     batches = spacy.util.minibatch(train_data, size=2)\n",
    "#     for batch in batches:\n",
    "#         for text, annotations in batch:\n",
    "#             # Create Example.\n",
    "#             doc = nlp.make_doc(text)\n",
    "#             example = Example.from_dict(doc, annotations)\n",
    "#             # Update the model.\n",
    "#             nlp.update([example], drop=0.3)\n",
    "\n",
    "# METHOD 2.\n",
    "# pipe_exceptions = [\"transformer\", \"ner\"]\n",
    "# unaffected_pipes = [pipe for pipe in nlp.pipe_names if pipe not in pipe_exceptions]\n",
    "# with nlp.select_pipes(disable=unaffected_pipes):\n",
    "#     for itn in range(20): # Set iterations.\n",
    "#         random.shuffle(train_data)\n",
    "#         losses = {}\n",
    "#         for text, annotations in train_data:\n",
    "#             doc = nlp.make_doc(text)\n",
    "#             example = Example.from_dict(doc, annotations)\n",
    "#             nlp.update([example], losses=losses)\n",
    "#         print(\"Iteration:\", itn + 1, \"Loss:\", losses)\n",
    "#\n",
    "#\n",
    "# METHOD 3.\n",
    "# if \"SOFTWARE\" not in ner.labels:\n",
    "#     ner.add_label(\"SOFTWARE\")\n",
    "#\n",
    "# pipe_exceptions = [\"transformer\", \"ner\", \"trf_wordpiecer\", \"trf_tok2vec\"]\n",
    "# unaffected_pipes = [pipe for pipe in nlp.pipe_names if pipe not in pipe_exceptions]\n",
    "#\n",
    "# with nlp.select_pipes(disable=unaffected_pipes):\n",
    "#     optimizer = nlp.resume_training()\n",
    "#     for iteration in range(10):  # Adjust the number of iterations as needed\n",
    "#         losses = {}\n",
    "#         for text, annotations in train_data:\n",
    "#             doc = nlp.make_doc(text)\n",
    "#             example = Example.from_dict(doc, annotations)\n",
    "#             nlp.update([example], losses=losses, sgd=optimizer)\n",
    "#         print(\"Iteration:\", iteration + 1, \"Loss:\", losses)\n",
    "\n",
    "# METHOD 4.\n",
    "from spacy.util import minibatch\n",
    "pipe_exceptions = [\"transformer\", \"ner\"]\n",
    "unaffected_pipes = [pipe for pipe in nlp.pipe_names if pipe not in pipe_exceptions]\n",
    "with nlp.select_pipes(disable=unaffected_pipes):\n",
    "    for itn in range(20): # Set iterations.\n",
    "        random.shuffle(train_data)\n",
    "        batches = minibatch(train_data, size=2)\n",
    "        losses = {}\n",
    "        for batch in batches:\n",
    "            examples = []\n",
    "            for text, annotations in train_data:\n",
    "                doc = nlp.make_doc(text)\n",
    "                example = Example.from_dict(doc, annotations)\n",
    "                examples.append(example)\n",
    "            nlp.update(examples=examples, losses=losses, drop=0.4)\n",
    "        print(\"Iteration:\", itn + 1, \"Loss:\", losses)\n",
    "\n",
    "nlp.to_disk(\"./en_web_core_trf_updated\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-12T23:26:33.278004836Z",
     "start_time": "2023-07-12T23:26:30.916531986Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"./en_web_core_trf_updated\")\n",
    "# cust_nlp = spacy.load(\"en_web_core_trf_updated\")\n",
    "# cust_nlp.replace_listeners(tok2vec_name=\"transformer\", pipe_name=\"ner\", listeners=[\"model.tok2vec\"])\n",
    "# nlp.add_pipe(factory_name=\"ner\", name=\"ner_custom\", source=cust_nlp, before=\"ner\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-12T23:26:45.244479109Z",
     "start_time": "2023-07-12T23:26:42.931180718Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['transformer', 'tagger', 'parser', 'attribute_ruler', 'lemmatizer', 'ner']\n",
      "('CARDINAL', 'DATE', 'EVENT', 'FAC', 'GPE', 'LANGUAGE', 'LAW', 'LOC', 'MONEY', 'NORP', 'ORDINAL', 'ORG', 'PERCENT', 'PERSON', 'PRODUCT', 'QUANTITY', 'SECURITY', 'SOFTWARE', 'TIME', 'WORK_OF_ART')\n"
     ]
    }
   ],
   "source": [
    "print(nlp.pipe_names)\n",
    "print(nlp.get_pipe(\"ner\").labels)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-12T23:26:46.647180017Z",
     "start_time": "2023-07-12T23:26:46.634960209Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entities []\n",
      "Tokens [('Baird', '', 2), ('TrustDesk', '', 2), ('Migration', '', 2), ('to', '', 2), ('OneDrive', '', 2), ('-', '', 2), ('Error', '', 2), ('migrating', '', 2)]\n",
      "Entities [('Microsoft', 'ORG'), ('Outlook', 'PRODUCT')]\n",
      "Tokens [('Microsoft', 'ORG', 3), ('Outlook', 'PRODUCT', 3), ('is', '', 2), ('crashing', '', 2)]\n",
      "Entities []\n",
      "Tokens [('Microsoft', '', 2), ('Excel', '', 2), ('throws', '', 2), ('error', '', 2), ('0xC0000142', '', 2)]\n",
      "Entities [('Windows Defender', 'PRODUCT'), ('this morning', 'TIME')]\n",
      "Tokens [('Windows', 'PRODUCT', 3), ('Defender', 'PRODUCT', 1), ('blocked', '', 2), ('my', '', 2), ('internet', '', 2), ('access', '', 2), ('this', 'TIME', 3), ('morning', 'TIME', 1), (',', '', 2), ('can', '', 2), ('you', '', 2), ('take', '', 2), ('a', '', 2), ('look', '', 2), ('?', '', 2)]\n",
      "Entities [('Microsoft Teams', 'ORG')]\n",
      "Tokens [('Microsoft', 'ORG', 3), ('Teams', 'ORG', 1), ('is', '', 2), ('crashing', '', 2)]\n",
      "Entities [('BitLocker', 'ORG')]\n",
      "Tokens [('BitLocker', 'ORG', 3), ('malfunctioned', '', 2), ('upon', '', 2), ('scanning', '', 2), ('this', '', 2), ('morning', '', 2)]\n",
      "Entities [('Microsoft', 'ORG'), ('Office', 'PRODUCT')]\n",
      "Tokens [('I', '', 2), ('use', '', 2), ('Microsoft', 'ORG', 3), ('Office', 'PRODUCT', 3), ('for', '', 2), ('my', '', 2), ('daily', '', 2), ('work', '', 2)]\n",
      "Entities [('Windows Firewall', 'PRODUCT')]\n",
      "Tokens [('Please', '', 2), ('check', '', 2), ('to', '', 2), ('see', '', 2), ('if', '', 2), ('Windows', 'PRODUCT', 3), ('Firewall', 'PRODUCT', 1), ('is', '', 2), ('working', '', 2)]\n",
      "Entities [('DocuSign', 'ORG'), ('Veronica Fitzpatrick', 'PERSON')]\n",
      "Tokens [('Hello', '', 2), ('.', '', 2), ('My', '', 2), ('name', '', 2), ('is', '', 2), ('spelled', '', 2), ('incorrectly', '', 2), ('on', '', 2), ('the', '', 2), ('DocuSign', 'ORG', 3), ('application', '', 2), ('.', '', 2), ('It', '', 2), ('is', '', 2), ('Veronica', 'PERSON', 3), ('Fitzpatrick', 'PERSON', 1), (',', '', 2), ('some', '', 2), ('letters', '', 2), ('are', '', 2), ('all', '', 2), ('flipped', '', 2), ('around', '', 2), (' ', '', 2), ('backwards', '', 2), ('and', '', 2), ('I', '', 2), ('was', '', 2), ('hoping', '', 2), ('to', '', 2), ('have', '', 2), ('that', '', 2), ('fixed', '', 2), ('!', '', 2)]\n"
     ]
    }
   ],
   "source": [
    "for text, _ in train_data:\n",
    "    doc = nlp(text)\n",
    "    print(\"Entities\", [(ent.text, ent.label_) for ent in doc.ents])\n",
    "    print(\"Tokens\", [(t.text, t.ent_type_, t.ent_iob) for t in doc])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-12T23:26:49.533165745Z",
     "start_time": "2023-07-12T23:26:49.248815565Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "## Using EntityRuler() to directly classify new entities based on a set of rules.\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# config = {\"overwrite_ents\": True}\n",
    "# nlp.add_pipe(\"entity_ruler\", config=config)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from spacy.pipeline import EntityRuler\n",
    "rulerSoftwares = nlp.add_pipe(\"entity_ruler\", before=\"ner\")\n",
    "patterns = [\n",
    "    {\"label\": \"SOFTWARE\", \"pattern\": \"Windows\"},\n",
    "    {\"label\": \"SOFTWARE\", \"pattern\": \"Windows Defender\"},\n",
    "    {\"label\": \"SOFTWARE\", \"pattern\": \"Windows OneDrive\"},\n",
    "    {\"label\": \"SOFTWARE\", \"pattern\": \"BitLocker\"},\n",
    "    {\"label\": \"SOFTWARE\", \"pattern\": \"BitDefender\"},\n",
    "    {\"label\": \"SOFTWARE\", \"pattern\": \"OneDrive\"},\n",
    "    {\"label\": \"SOFTWARE\", \"pattern\": \"Windows Firewall\"},\n",
    "    {\"label\": \"SOFTWARE\", \"pattern\": \"Windows Server\"},\n",
    "    {\"label\": \"SOFTWARE\", \"pattern\": \"Microsoft Teams\"},\n",
    "    {\"label\": \"SOFTWARE\", \"pattern\": \"Microsoft\"},\n",
    "    {\"label\": \"SOFTWARE\", \"pattern\": \"DocuSign\"}\n",
    "]\n",
    "rulerSoftwares.add_patterns(patterns)\n",
    "print(nlp.pipe_names)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DocuSigns ORG\n"
     ]
    }
   ],
   "source": [
    "# nlp = spacy.load(\"en_web_core_trf_updated\")\n",
    "doc = nlp(\"DocuSigns is doing its job.\")\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.label_)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-12T23:26:54.567775857Z",
     "start_time": "2023-07-12T23:26:54.518513645Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pitney Bowes P2000 Series isn't able to connect with network\n",
      "Pitney PROPN compound\n",
      "Bowes PROPN compound\n",
      "P2000 PROPN compound\n",
      "Series PROPN nsubj\n",
      "is AUX ROOT\n",
      "n't PART neg\n",
      "able ADJ acomp\n",
      "to PART aux\n",
      "connect VERB xcomp\n",
      "with ADP prep\n",
      "network NOUN pobj\n"
     ]
    }
   ],
   "source": [
    "# Example of token generation for the first body of text.\n",
    "doc = nlp(df['short_description'][0])\n",
    "print(doc.text)\n",
    "for token in doc:\n",
    "    print(token.text, token.pos_, token.dep_) # Print: token, POS, syntactic dependency."
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-12T23:26:54.701724293Z",
     "start_time": "2023-07-12T23:26:54.659047953Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "outputs": [],
   "source": [
    "# Create function to add an article's tokens to `doc_list`.\n",
    "# Tokenize one time, then use that object for the subsequent accumulators.\n",
    "# Returns None many times.\n",
    "doc_list = []\n",
    "def to_doc_list(text):\n",
    "    doc_list.append(nlp(text))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-12T23:26:55.122307946Z",
     "start_time": "2023-07-12T23:26:55.113203364Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "outputs": [
    {
     "data": {
      "text/plain": "0     None\n1     None\n2     None\n3     None\n4     None\n5     None\n6     None\n7     None\n8     None\n9     None\n10    None\n11    None\n12    None\n13    None\n14    None\n15    None\n16    None\n17    None\n18    None\nName: short_description, dtype: object"
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Takes time to generate tokens from each cell's fulltext.\n",
    "df['short_description'].apply(to_doc_list)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-12T23:26:56.066983030Z",
     "start_time": "2023-07-12T23:26:55.390295784Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "outputs": [
    {
     "data": {
      "text/plain": "0     (Pitney, Bowes, P2000, Series, is, n't, able, ...\n1       (Can, not, connect, to, in, -, office, Desktop)\n2             (Brittany, Tyler, -, Fixed, Income, Desk)\n3     (Break, /, fix, Docking, station, not, providi...\n4     (Baird, TrustDesk, Migration, to, OneDrive, -,...\n5                      (Deposit, Edge, Account, Unlock)\n6     (MFA, Authentication, Loop, -, Unable, to, acc...\n7     (Beta, report, ran, out, of, paper, ., Needs, ...\n8         (Laptop, not, charging, on, Docking, Station)\n9               (Mac, not, connecting, to, Guest, wifi)\n10    (RJ, Edgerly, needs, his, Outlook, inbox, rest...\n11    (Hello, ., My, name, is, spelled, incorrectly,...\n12    (Custom, example, :, Bryce, Townsend, tried, t...\n13    (Custom, example, :, Aditya, Patel, needs, a, ...\n14    (Custom, example, :, Stanislav, Oliynyk, needs...\n15    (Custom, example, :, Parasol, needs, to, have,...\n16    (Please, call, +49, 30, 901820, to, resolve, t...\n17    (Please, call, +1, -, 568, -, 445, -, 8792, to...\n18    (Is, 989, -, 456, -, 122, the, right, number, ...\ndtype: object"
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Assign `doc_list` to `doc_series` as a Series object.\n",
    "doc_series = pd.Series(doc_list)\n",
    "doc_series"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-12T23:26:57.428083405Z",
     "start_time": "2023-07-12T23:26:57.420987591Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pitney Bowes P2000 Series isn't able to connect with network\n",
      "Pitney Bowes P2000 Series is n't able to connect with network \n",
      "\n",
      "Cannot connect to in-office Desktop\n",
      "Can not connect to in- office Desktop \n",
      "\n",
      "Brittany Tyler - Fixed Income Desk\n",
      "<PERSON> <PERSON>- Fixed Income Desk \n",
      "\n",
      "Break/fix Docking station not providing monitor display or network\n",
      "Break / fix Docking station not providing monitor display or network \n",
      "\n",
      "Baird TrustDesk Migration to OneDrive - Error migrating \n",
      "Baird TrustDesk Migration to OneDrive- Error migrating \n",
      "\n",
      "Deposit Edge Account Unlock\n",
      "Deposit Edge Account Unlock \n",
      "\n",
      "MFA Authentication Loop - Unable to access email\n",
      "MFA Authentication Loop- Unable to access email \n",
      "\n",
      "Beta report ran out of paper. Needs a restart.\n",
      "Beta report ran out of paper. Needs a restart. \n",
      "\n",
      "Laptop not charging on Docking Station\n",
      "Laptop not charging on Docking Station \n",
      "\n",
      "Mac not connecting to Guest wifi\n",
      "Mac not connecting to Guest wifi \n",
      "\n",
      "RJ Edgerly needs his Outlook inbox restored\n",
      "<PERSON> <PERSON> needs his <PRODUCT> inbox restored \n",
      "\n",
      "Hello. My name is spelled incorrectly on the DocuSign application. It is Veronica Fitzpatrick, some letters are all flipped around  backwards and I was hoping to have that fixed!\n",
      "Hello. My name is spelled incorrectly on the DocuSign application. It is <PERSON> <PERSON>, some letters are all flipped around   backwards and I was hoping to have that fixed! \n",
      "\n",
      "Custom example: Bryce Townsend tried to run the program 6 times earlier to no avail. Can you please check? It’s costing us $2 per day! A million thanks. You are 100% awesome.\n",
      "Custom example: <PERSON> <PERSON> tried to run the program <CARDINAL> times earlier to no avail. Can you please check? It ’s costing us $ <MONEY> per day! <CARDINAL> <CARDINAL> thanks. You are <PERCENT> <PERCENT> awesome. \n",
      "\n",
      "Custom example: Aditya Patel needs a new machine, so does Emilio Hernandez.\n",
      "Custom example: <PERSON> <PERSON> needs a new machine, so does <PERSON> <PERSON>. \n",
      "\n",
      "Custom example: Stanislav Oliynyk needs a new machine, so does Takeshi Ueda\n",
      "Custom example: <PERSON> <PERSON> needs a new machine, so does <PERSON> <PERSON> \n",
      "\n",
      "Custom example: Parasol needs to have her new workstation set up – please follow up. Thanks.\n",
      "Custom example: <PERSON> needs to have her new workstation set up– please follow up. Thanks. \n",
      "\n",
      "Please call +49 30 901820 to resolve the issue in Germany – thanks!\n",
      "Please call +49 30 901820 to resolve the issue in Germany– thanks! \n",
      "\n",
      "Please call +1-568-445-8792 to reach our help line.\n",
      "Please call +1 - 568 - 445 - 8792 to reach our help line. \n",
      "\n",
      "Is 989-456-122 the right number for the help desk? Thanks!\n",
      "Is 989 - 456 - 122 the right number for the help desk? Thanks! \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for doc in doc_series:\n",
    "    print(doc)\n",
    "    filtered_string = \"\"\n",
    "    for token in doc:\n",
    "        if token.ent_type_ in ['PERSON', 'PRODUCT', 'MONEY', 'CARDINAL', 'QUANTITY', 'PERCENT', 'SOFTWARE', 'SECURITY']:\n",
    "            new_token = \" <{}>\".format(token.ent_type_)\n",
    "        # elif token.pos_ in ['PROPN']:\n",
    "        #     new_token = \" <PROPN>\"\n",
    "        # elif token.pos_ in ['PROPN', 'NUM']:\n",
    "        #     new_token = \" <{}>\".format(token.ent_type_)\n",
    "        elif token.pos_ == \"PUNCT\":\n",
    "            new_token = token.text\n",
    "        else:\n",
    "            new_token = \" {}\".format(token.text)\n",
    "        filtered_string += new_token\n",
    "    filtered_string = filtered_string[1:]\n",
    "    print(filtered_string, '\\n')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-12T23:26:57.765219663Z",
     "start_time": "2023-07-12T23:26:57.752639287Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "outputs": [],
   "source": [
    "# Remove phone numbers.\n",
    "import re\n",
    "def remove_phone_numbers(text):\n",
    "    pattern = r\"\\b(?:\\+?\\d{1,3}[-.])?\\(?\\d{1,3}\\)?[-.\\s]?\\d{1,3}[-.\\s]?\\d{1,4}[-.\\s]?\\d{1,9}\\b\"\n",
    "    return re.sub(pattern, \"<PHONE NUMBER>\", text)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-12T23:30:11.913609661Z",
     "start_time": "2023-07-12T23:30:11.911614110Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "outputs": [
    {
     "data": {
      "text/plain": "0     Pitney Bowes P2000 Series isn't able to connec...\n1                   Cannot connect to in-office Desktop\n2                    Brittany Tyler - Fixed Income Desk\n3     Break/fix Docking station not providing monito...\n4     Baird TrustDesk Migration to OneDrive - Error ...\n5                           Deposit Edge Account Unlock\n6      MFA Authentication Loop - Unable to access email\n7        Beta report ran out of paper. Needs a restart.\n8                Laptop not charging on Docking Station\n9                      Mac not connecting to Guest wifi\n10          RJ Edgerly needs his Outlook inbox restored\n11    Hello. My name is spelled incorrectly on the D...\n12    Custom example: Bryce Townsend tried to run th...\n13    Custom example: Aditya Patel needs a new machi...\n14    Custom example: Stanislav Oliynyk needs a new ...\n15    Custom example: Parasol needs to have her new ...\n16    Please call +<PHONE NUMBER> to resolve the iss...\n17    Please call +<PHONE NUMBER> to reach our help ...\n18    Is <PHONE NUMBER> the right number for the hel...\nName: short_description, dtype: object"
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['short_description'].apply(remove_phone_numbers)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-12T23:30:12.098109438Z",
     "start_time": "2023-07-12T23:30:12.094703249Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Remove email addresses.\n",
    "def remove_email_addresses(text):\n",
    "    pattern = r\"\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Za-z]{2,}\\b\"\n",
    "    return re.sub(pattern, \"<EMAIL>\", text)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "# Check NER results.\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "Dictionary accumulator for entities based on entity type\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "ent_dict = {}\n",
    "def count_ent(doc):\n",
    "    for ent in doc.ents:\n",
    "        if ent.label_ not in ent_dict:\n",
    "            ent_dict[ent.label_] = 1\n",
    "        else:\n",
    "            ent_dict[ent.label_]+=1\n",
    "doc_series.apply(count_ent)\n",
    "ent_dict"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('CARDINAL', 'DATE', 'EVENT', 'FAC', 'GPE', 'LANGUAGE', 'LAW', 'LOC', 'MONEY', 'NORP', 'ORDINAL', 'ORG', 'PERCENT', 'PERSON', 'PRODUCT', 'QUANTITY', 'SECURITY', 'SOFTWARE', 'TIME', 'WORK_OF_ART')\n"
     ]
    }
   ],
   "source": [
    "print(nlp.get_pipe(\"ner\").labels)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-12T23:00:00.849449698Z",
     "start_time": "2023-07-12T23:00:00.839049964Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "#### PERSON.\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "ent_dict = {}\n",
    "def count_person(doc):\n",
    "    for ent in doc.ents:\n",
    "        if ent.label_ == 'PERSON':\n",
    "            if ent.text not in ent_dict:\n",
    "                ent_dict[ent.text]=1\n",
    "            else:\n",
    "                ent_dict[ent.text]+=1\n",
    "doc_series.apply(count_person)\n",
    "sorted(ent_dict.items(), key=lambda x: x[1], reverse=True)[0:10]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "&nbsp;\n",
    "#### Save out above entity list + counts as a pd.Sereies() object.\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "output_dir = r'./PII'"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "ner_obj = sorted(ent_dict.items(), key=lambda x: x[1], reverse=True) # specify which dict to save\n",
    "ner_obj = pd.Series(ner_obj)\n",
    "ner_obj.to_csv(output_dir + 'ner_obj.csv', sep=';', encoding='utf-8-sig')"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
